package main

import (
	"fmt"
	"math"

	"cure_test/pkg/activation_he"
	"cure_test/pkg/he"

	"github.com/tuneinsight/lattigo/v6/core/rlwe"
	"github.com/tuneinsight/lattigo/v6/schemes/ckks"
)

func main() {
	fmt.Println("=== Homomorphic Activation Functions Demo ===")

	// Step 1: Setup HE parameters and keys
	fmt.Println("\n1. Setting up HE parameters and keys...")
	params, err := he.GetCKKSParameters(he.DefaultSet)
	if err != nil {
		panic(fmt.Sprintf("Failed to get CKKS parameters: %v", err))
	}

	kgen := he.KeyGenerator(params)
	// In Lattigo v6, we need to create the keys first
	sk := rlwe.NewSecretKey(params.Parameters)
	pk := rlwe.NewPublicKey(params.Parameters)
	rlk := rlwe.NewRelinearizationKey(params.Parameters)
	
	// Generate the keys
	kgen.GenSecretKey(sk)
	kgen.GenPublicKey(sk, pk)
	kgen.GenRelinearizationKey(sk, rlk)

	// Step 2: Create encoder, encryptor, decryptor, and evaluator
	fmt.Println("2. Creating encoder, encryptor, decryptor, and evaluator...")
	encoder := he.NewEncoder(params)
	encryptor := he.NewEncryptor(params, pk)
	decryptor := he.NewDecryptor(params, sk)
	
	// Create evaluation key set with the relinearization key
	evk := rlwe.NewMemEvaluationKeySet(rlk)
	evaluator := he.NewEvaluator(params, evk)

	// Step 3: Prepare test inputs
	fmt.Println("3. Preparing test inputs...")
	inputs := []float64{-3.0, -2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0}

	// Step 4: Demonstrate each activation function
	fmt.Println("\n=== ReLU Activation ===")
	demonstrateActivation(
		"ReLU",
		activation_he.ReLU,
		activation_he.EvalReLUHE,
		nil,
		inputs,
		encoder,
		encryptor,
		decryptor,
		evaluator,
		params,
	)

	fmt.Println("\n=== Sigmoid Activation ===")
	demonstrateActivation(
		"Sigmoid",
		activation_he.Sigmoid,
		activation_he.EvalSigmoidHE,
		nil,
		inputs,
		encoder,
		encryptor,
		decryptor,
		evaluator,
		params,
	)

	fmt.Println("\n=== Tanh Activation ===")
	demonstrateActivation(
		"Tanh",
		activation_he.Tanh,
		activation_he.EvalTanhHE,
		nil,
		inputs,
		encoder,
		encryptor,
		decryptor,
		evaluator,
		params,
	)

	// Step 5: Demonstrate a simple neural network layer
	fmt.Println("\n=== Simple Neural Network Layer ===")
	demonstrateNeuralNetworkLayer(
		inputs,
		encoder,
		encryptor,
		decryptor,
		evaluator,
		params,
	)

	// Step 6: Print polynomial approximation information
	fmt.Println("\n=== Polynomial Approximation Information ===")

	// ReLU polynomial info
	maxError, point := activation_he.ComputeMaxApproximationError(
		activation_he.ReLU,
		activation_he.DefaultReLUConfig.Coefficients,
		activation_he.DefaultReLUConfig.Interval,
		1000,
	)
	fmt.Printf("ReLU Polynomial (degree %d):\n", len(activation_he.DefaultReLUConfig.Coefficients)-1)
	fmt.Printf("  Formula: %s\n", activation_he.FormatPolynomialString(activation_he.DefaultReLUConfig.Coefficients, 6))
	fmt.Printf("  Interval: [%.1f, %.1f]\n", activation_he.DefaultReLUConfig.Interval[0], activation_he.DefaultReLUConfig.Interval[1])
	fmt.Printf("  Max Error: %.6f at x = %.2f\n\n", maxError, point)

	// Sigmoid polynomial info
	maxError, point = activation_he.ComputeMaxApproximationError(
		activation_he.Sigmoid,
		activation_he.DefaultSigmoidConfig.Coefficients,
		activation_he.DefaultSigmoidConfig.Interval,
		1000,
	)
	fmt.Printf("Sigmoid Polynomial (degree %d):\n", len(activation_he.DefaultSigmoidConfig.Coefficients)-1)
	fmt.Printf("  Formula: %s\n", activation_he.FormatPolynomialString(activation_he.DefaultSigmoidConfig.Coefficients, 6))
	fmt.Printf("  Interval: [%.1f, %.1f]\n", activation_he.DefaultSigmoidConfig.Interval[0], activation_he.DefaultSigmoidConfig.Interval[1])
	fmt.Printf("  Max Error: %.6f at x = %.2f\n\n", maxError, point)

	// Tanh polynomial info
	maxError, point = activation_he.ComputeMaxApproximationError(
		activation_he.Tanh,
		activation_he.DefaultTanhConfig.Coefficients,
		activation_he.DefaultTanhConfig.Interval,
		1000,
	)
	fmt.Printf("Tanh Polynomial (degree %d):\n", len(activation_he.DefaultTanhConfig.Coefficients)-1)
	fmt.Printf("  Formula: %s\n", activation_he.FormatPolynomialString(activation_he.DefaultTanhConfig.Coefficients, 6))
	fmt.Printf("  Interval: [%.1f, %.1f]\n", activation_he.DefaultTanhConfig.Interval[0], activation_he.DefaultTanhConfig.Interval[1])
	fmt.Printf("  Max Error: %.6f at x = %.2f\n", maxError, point)
}

// demonstrateActivation demonstrates a single activation function
func demonstrateActivation(
	name string,
	plainFunc func(float64) float64,
	heFunc func(*rlwe.Ciphertext, *ckks.Evaluator, ckks.Parameters, *activation_he.ActivationConfig) (*rlwe.Ciphertext, error),
	config *activation_he.ActivationConfig,
	inputs []float64,
	encoder *ckks.Encoder,
	encryptor *rlwe.Encryptor,
	decryptor *rlwe.Decryptor,
	evaluator *ckks.Evaluator,
	params ckks.Parameters,
) {
	fmt.Printf("Input | Expected | HE Result | Error\n")
	fmt.Printf("------|----------|-----------|------\n")

	for _, x := range inputs {
		// Compute expected result in plaintext
		expected := plainFunc(x)

		// Encode and encrypt the input
		plaintext := encoder.EncodeNew([]float64{x}, params.MaxLevel())
		ciphertext := encryptor.EncryptNew(plaintext)

		// Apply activation function homomorphically
		resultCt, err := heFunc(ciphertext, evaluator, params, config)
		if err != nil {
			fmt.Printf("%.2f | %.6f | ERROR: %v\n", x, expected, err)
			continue
		}

		// Decrypt and decode the result
		resultPlaintext := decryptor.DecryptNew(resultCt)
		
		// In Lattigo v6, we need to create the output slice first
		resultValues := make([]float64, 1)
		encoder.Decode(resultPlaintext, resultValues)
		actual := resultValues[0]

		// Calculate error
		error := math.Abs(expected - actual)

		fmt.Printf("%.2f | %.6f | %.6f | %.6f\n", x, expected, actual, error)
	}
}

// demonstrateNeuralNetworkLayer demonstrates a simple neural network layer with activation
func demonstrateNeuralNetworkLayer(
	inputs []float64,
	encoder *ckks.Encoder,
	encryptor *rlwe.Encryptor,
	decryptor *rlwe.Decryptor,
	evaluator *ckks.Evaluator,
	params ckks.Parameters,
) {
	// Define a simple neural network layer: y = ReLU(w*x + b)
	weight := 1.5
	bias := -1.0

	fmt.Printf("Neural Network Layer: y = ReLU(%.2f*x + %.2f)\n\n", weight, bias)
	fmt.Printf("Input | Linear Output | Expected ReLU | HE Result | Error\n")
	fmt.Printf("------|--------------|--------------|-----------|------\n")

	for _, x := range inputs {
		// Compute expected output in plaintext
		linearOutput := weight*x + bias
		expected := activation_he.ReLU(linearOutput)

		// Encode and encrypt the input
		plaintext := encoder.EncodeNew([]float64{x}, params.MaxLevel())
		ciphertext := encryptor.EncryptNew(plaintext)

		// Perform linear transformation: w*x + b
		weightedCt, err := evaluator.MulNew(ciphertext, weight)
		if err != nil {
			fmt.Printf("%.2f | %.6f | %.6f | ERROR: %v\n", x, linearOutput, expected, err)
			continue
		}

		// Create a plaintext with the bias value
		biasPt := ckks.NewPlaintext(params, weightedCt.Level())
		encoder.Encode([]float64{bias}, biasPt)
		
		// Add the bias as a plaintext
		err = evaluator.Add(weightedCt, biasPt, weightedCt)
		if err != nil {
			fmt.Printf("%.2f | %.6f | %.6f | ERROR: %v\n", x, linearOutput, expected, err)
			continue
		}

		// Apply ReLU activation
		activatedCt, err := activation_he.EvalReLUHE(weightedCt, evaluator, params, nil)
		if err != nil {
			fmt.Printf("%.2f | %.6f | %.6f | ERROR: %v\n", x, linearOutput, expected, err)
			continue
		}

		// Decrypt and decode the result
		resultPlaintext := decryptor.DecryptNew(activatedCt)
		
		// In Lattigo v6, we need to create the output slice first
		resultValues := make([]float64, 1)
		encoder.Decode(resultPlaintext, resultValues)
		actual := resultValues[0]

		// Calculate error
		error := math.Abs(expected - actual)

		fmt.Printf("%.2f | %.6f | %.6f | %.6f | %.6f\n",
			x, linearOutput, expected, actual, error)
	}
}
